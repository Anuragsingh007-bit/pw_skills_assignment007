{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32549602",
   "metadata": {},
   "source": [
    "# Neural Network — A Simple Perceptron\n",
    "\n",
    "*Theory + Practical in one Colab notebook.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2fd98",
   "metadata": {},
   "source": [
    "## Theory — Short Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16c466",
   "metadata": {},
   "source": [
    "**Q1. What is deep learning, and how is it connected to artificial intelligence?**\n",
    "\n",
    "Deep learning is a subset of machine learning (which in turn is a subset of AI) that uses multi-layered neural networks to learn hierarchical representations of data. It enables machines to learn complex patterns directly from raw data with minimal feature engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0504289",
   "metadata": {},
   "source": [
    "**Q2. What is a neural network, and what are the different types of neural networks?**\n",
    "\n",
    "A neural network is a set of layers of interconnected nodes (neurons) that transform input data through weighted connections and activation functions to produce outputs. Common types include: feedforward (MLP), convolutional (CNN), recurrent (RNN, LSTM/GRU), autoencoders, and transformers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f953525",
   "metadata": {},
   "source": [
    "**Q3. What is the mathematical structure of a neural network?**\n",
    "\n",
    "A neural network is composed of layers where each layer performs an affine transformation followed by a non-linear activation: z = W x + b, a = φ(z). Training adjusts weights W and biases b to minimize a loss function over data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d3c95",
   "metadata": {},
   "source": [
    "**Q4. What is an activation function, and why is it essential in neural networks?**\n",
    "\n",
    "An activation function introduces non-linearity to allow the network to learn complex relationships. Without activation functions, the network would be equivalent to a single linear transformation regardless of depth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555e000",
   "metadata": {},
   "source": [
    "**Q5. Could you list some common activation functions used in neural networks?**\n",
    "\n",
    "Common activations: Sigmoid, Tanh, ReLU (Rectified Linear Unit), Leaky ReLU, ELU, Softmax (for multiclass output).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3449c6e",
   "metadata": {},
   "source": [
    "**Q6. What is a multilayer neural network?**\n",
    "\n",
    "A multilayer neural network (also called a multilayer perceptron) has one or more hidden layers between input and output. Each hidden layer allows the model to learn higher-level features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755eedb",
   "metadata": {},
   "source": [
    "**Q7. What is a loss function, and why is it crucial for neural network training?**\n",
    "\n",
    "A loss function measures the difference between predicted outputs and true targets. Training aims to minimize this loss using optimization algorithms; the choice of loss depends on the task (e.g., cross-entropy for classification, MSE for regression).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc483a64",
   "metadata": {},
   "source": [
    "**Q8. What are some common types of loss functions?**\n",
    "\n",
    "Common losses: Mean Squared Error (MSE), Mean Absolute Error (MAE), Binary Cross-Entropy, Categorical Cross-Entropy, Hinge loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096ef00",
   "metadata": {},
   "source": [
    "**Q9. How does a neural network learn?**\n",
    "\n",
    "It learns through iterative optimization: forward pass computes predictions and loss; backward pass (backpropagation) computes gradients of loss w.r.t parameters; an optimizer updates parameters using gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fe71e",
   "metadata": {},
   "source": [
    "**Q10. What is an optimizer in neural networks, and why is it necessary?**\n",
    "\n",
    "An optimizer updates network parameters to minimize loss using gradients. It controls step sizes and can include momentum or adaptive learning rates to improve convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1fb4ca",
   "metadata": {},
   "source": [
    "**Q11. Could you briefly describe some common optimizers?**\n",
    "\n",
    "SGD (Stochastic Gradient Descent), SGD with momentum, Adam (adaptive moment estimation), RMSprop, Adagrad. Adam is popular due to adaptive learning rates and good default performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa1f78",
   "metadata": {},
   "source": [
    "**Q12. Can you explain forward and backward propagation in a neural network?**\n",
    "\n",
    "Forward propagation passes input through layers to compute output. Backward propagation (backprop) computes gradients of loss wrt parameters using chain rule, propagating errors from output back to earlier layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e405d",
   "metadata": {},
   "source": [
    "**Q13. What is weight initialization, and how does it impact training?**\n",
    "\n",
    "Weight initialization sets initial parameter values. Proper initialization (e.g., Xavier/Glorot, He initialization) helps avoid vanishing/exploding gradients and leads to faster, more stable training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280cabd",
   "metadata": {},
   "source": [
    "**Q14. What is the vanishing gradient problem in deep learning?**\n",
    "\n",
    "When gradients become extremely small while backpropagating through many layers, early layers learn very slowly. This can make training deep networks difficult, particularly with sigmoid/tanh activations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b37f1",
   "metadata": {},
   "source": [
    "**Q15. What is the exploding gradient problem?**\n",
    "\n",
    "When gradients grow very large during backpropagation, causing unstable updates and possible divergence. Techniques like gradient clipping and careful initialization help mitigate this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009d6c7",
   "metadata": {},
   "source": [
    "## Practical — Keras / TensorFlow Examples\n",
    "\n",
    "Run these cells in Google Colab (ensure `tensorflow` is installed). Each example uses a small dataset or synthetic data and short epochs for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 1: Simple perceptron-like model for binary classification (toy example)\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Simple dataset (logical OR)\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
    "model.compile(optimizer=SGD(learning_rate=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=50, verbose=0)\n",
    "print('Final loss:', history.history['loss'][-1])\n",
    "print('Accuracy:', history.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e085eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 2: MLP with one hidden layer\n",
    "from tensorflow.keras.layers import Dense\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(8, activation='relu', input_shape=(2,)))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "h2 = model2.fit(X, y, epochs=50, verbose=0)\n",
    "print('Accuracy (one hidden layer):', h2.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff394547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 3: Xavier (Glorot) initialization example\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(8, activation='relu', kernel_initializer=GlorotUniform(), input_shape=(2,)))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "print('Model with Glorot initialized layers constructed (no training here)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73578dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 4: Different activation functions in a simple model\n",
    "model_act = Sequential()\n",
    "model_act.add(Dense(8, activation='tanh', input_shape=(2,)))\n",
    "model_act.add(Dense(1, activation='sigmoid'))\n",
    "model_act.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "print('Model with tanh activation constructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 5: Dropout example\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model_drop = Sequential()\n",
    "model_drop.add(Dense(16, activation='relu', input_shape=(2,)))\n",
    "model_drop.add(Dropout(0.5))\n",
    "model_drop.add(Dense(1, activation='sigmoid'))\n",
    "model_drop.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "print('Model with dropout constructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515498f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 6: Manual forward propagation for a 2-layer network (numpy)\n",
    "import numpy as np\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = 1/(1+np.exp(-z2))  # sigmoid\n",
    "    return a2\n",
    "\n",
    "# small random weights\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(2,4)\n",
    "b1 = np.zeros(4)\n",
    "W2 = np.random.randn(4,1)\n",
    "b2 = np.zeros(1)\n",
    "print('Forward output sample:', forward(np.array([[1,0]]), W1, b1, W2, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60041386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 7: BatchNormalization example\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "model_bn = Sequential()\n",
    "model_bn.add(Dense(16, input_shape=(2,)))\n",
    "model_bn.add(BatchNormalization())\n",
    "model_bn.add(Dense(1, activation='sigmoid'))\n",
    "model_bn.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "print('Model with batch normalization constructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 8: Visualize training (example using model2 history)\n",
    "import matplotlib.pyplot as plt\n",
    "h = h2.history\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(h['loss']); plt.title('Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(h['accuracy']); plt.title('Accuracy')\n",
    "plt.tight_layout()\n",
    "print('Plotted loss and accuracy (run in Colab to display)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403deed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 9: Gradient clipping using optimizer arguments\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.01, clipnorm=1.0)  # or clipvalue\n",
    "model_clip = Sequential([Dense(8, activation='relu', input_shape=(2,)), Dense(1, activation='sigmoid')])\n",
    "model_clip.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "print('Compiled model with gradient clipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4699d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 10: Custom loss function example (mean absolute percentage error variant)\n",
    "import tensorflow.keras.backend as K\n",
    "def custom_mape(y_true, y_pred):\n",
    "    return K.mean(K.abs((y_true - y_pred) / (K.clip(K.abs(y_true), K.epsilon(), None)))) * 100\n",
    "\n",
    "model_cl = Sequential([Dense(8, activation='relu', input_shape=(2,)), Dense(1)])\n",
    "model_cl.compile(optimizer='adam', loss=custom_mape)\n",
    "print('Model compiled with custom loss (custom_mape)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2065a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 11: Visualize model structure (text-based)\n",
    "model2.summary()  # prints layer info; for graphical plot use plot_model (requires pydot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e7628",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Notebook prepared for student assignment: theory + practical examples. Run in Google Colab. Short epochs and tiny datasets used for quick demonstration.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

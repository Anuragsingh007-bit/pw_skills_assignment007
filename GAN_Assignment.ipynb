{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7588140",
   "metadata": {},
   "source": [
    "# GANs — Theory & Practical\n",
    "\n",
    "This Colab-ready notebook contains both **theory** (concise, student-style answers) and **practical** Keras/TensorFlow examples. Code cells are runnable in Google Colab. I kept training epochs small so cells run quickly for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657f3ae",
   "metadata": {},
   "source": [
    "## Theory — GAN Questions and Short Answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1914d2",
   "metadata": {},
   "source": [
    "**Q1. What does GAN stand for, and what is its main purpose?**\n",
    "\n",
    "GAN stands for **Generative Adversarial Network**. Its main purpose is to learn a generator that can produce new data samples (e.g., images) that are similar to real data, by training a generator and a discriminator in an adversarial setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ef829",
   "metadata": {},
   "source": [
    "**Q2. Explain the concept of the 'discriminator' in GANs.**\n",
    "\n",
    "The discriminator is a neural network trained to distinguish real samples from fake (generated) samples. It outputs a probability (or score) indicating whether an input is real. The discriminator provides feedback to the generator via gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41adfc12",
   "metadata": {},
   "source": [
    "**Q3. How does a GAN work?**\n",
    "\n",
    "A GAN has two networks: a generator G that maps random noise z -> x_fake, and a discriminator D that scores inputs. Training alternates: D learns to classify real vs fake, and G learns to produce fakes that fool D. This adversarial game improves generator samples over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423cacb",
   "metadata": {},
   "source": [
    "**Q4. What is the generator's role in a GAN?**\n",
    "\n",
    "The generator creates synthetic samples from noise (and possibly conditional inputs). Its goal is to produce outputs that are indistinguishable from real data according to the discriminator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71574d8",
   "metadata": {},
   "source": [
    "**Q5. What is the loss function used in the training of GANs?**\n",
    "\n",
    "The original GAN uses a minimax loss: D maximizes log(D(x)) + log(1 - D(G(z))) while G minimizes log(1 - D(G(z))). In practice, G often maximizes log(D(G(z))) (non-saturating loss) for better gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581641c4",
   "metadata": {},
   "source": [
    "**Q6. What is the difference between a WGAN and a traditional GAN?**\n",
    "\n",
    "WGAN (Wasserstein GAN) replaces the discriminator with a critic that estimates the Wasserstein distance between real and generated distributions. WGAN uses a different loss (Wasserstein loss) and enforces Lipschitz continuity (originally via weight clipping, later via gradient penalty) to improve training stability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de345b",
   "metadata": {},
   "source": [
    "**Q7. How does the training of the generator differ from that of the discriminator?**\n",
    "\n",
    "Training alternates: the discriminator (or critic) is trained for some steps to improve classification/Scoring, then the generator is trained to improve its outputs using gradients from the discriminator. Hyperparameters (like multiple critic steps) may differ between the two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63fb0d",
   "metadata": {},
   "source": [
    "**Q8. What is a DCGAN, and how is it different from a traditional GAN?**\n",
    "\n",
    "DCGAN (Deep Convolutional GAN) uses convolutional and transposed-convolutional layers in the discriminator and generator respectively, with architectural best-practices (BatchNorm, ReLU/LeakyReLU) tailored for stable image generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764bc98",
   "metadata": {},
   "source": [
    "**Q9. Explain the concept of 'controllable generation' in the context of GANs.**\n",
    "\n",
    "Controllable generation means conditioning the generator on external inputs (labels, attributes, or latent codes) so you can control aspects of generated samples — e.g., class-conditional GANs, style codes in StyleGAN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c3ab6",
   "metadata": {},
   "source": [
    "**Q10. What is the primary goal of training a GAN?**\n",
    "\n",
    "The primary goal is to train a generator whose output distribution matches the real data distribution closely, so generated samples are realistic and diverse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23881621",
   "metadata": {},
   "source": [
    "**Q11. What are the limitations of GANs?**\n",
    "\n",
    "GANs can be unstable to train, suffer from mode collapse (lack of diversity), require careful architecture and hyperparameter tuning, and evaluation metrics are not straightforward (FID/IS approximate).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9a5f9",
   "metadata": {},
   "source": [
    "**Q12. What are StyleGANs, and what makes them unique?**\n",
    "\n",
    "StyleGAN family introduces a style-based generator architecture that injects learned style vectors at multiple levels via adaptive instance normalization; they produce high-quality, controllable, and disentangled image synthesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ffc8a1",
   "metadata": {},
   "source": [
    "**Q13. What is the role of noise in a GAN?**\n",
    "\n",
    "Noise (latent vector z) is the input to the generator and provides randomness to produce varied outputs. Structured latent spaces enable interpolation and semantic manipulations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f5aba",
   "metadata": {},
   "source": [
    "**Q14. How does the loss function in a WGAN improve training stability?**\n",
    "\n",
    "Wasserstein loss approximates an earth-mover distance which correlates better with sample quality and provides smoother gradients. When coupled with gradient penalty (WGAN-GP) or proper Lipschitz enforcement, it stabilizes training and reduces mode collapse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5f713",
   "metadata": {},
   "source": [
    "**Q15. Describe the architecture of a typical GAN.**\n",
    "\n",
    "A typical GAN has: a generator (series of upsampling/transpose-conv layers, batchnorm, activations) and a discriminator (downsampling conv layers, LeakyReLU, and a final sigmoid/linear output). DCGANs follow specific design rules for stability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbff407",
   "metadata": {},
   "source": [
    "**Q16. What challenges do GANs face during training, and how can they be addressed?**\n",
    "\n",
    "Challenges: instability, mode collapse, vanishing gradients. Remedies: alternative losses (WGAN), gradient penalty, spectral normalization, two-time-scale updates, architectural choices (BatchNorm/InstanceNorm), and regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a7ef1",
   "metadata": {},
   "source": [
    "**Q17. How does DCGAN help improve image generation in GANs?**\n",
    "\n",
    "DCGAN uses convolutional architectures, BatchNorm, and ReLU/LeakyReLU activations which empirically lead to more stable training and higher-quality images for common image sizes (e.g., 64x64).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc4cea",
   "metadata": {},
   "source": [
    "**Q18. What are the key differences between a traditional GAN and a StyleGAN?**\n",
    "\n",
    "StyleGAN introduces mapping network and style injection (AdaIN), noise inputs at each layer for stochastic detail, and progressive architecture adjustments. This yields finer control over synthesis and higher fidelity outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c37be14",
   "metadata": {},
   "source": [
    "**Q19. How does the discriminator decide whether an image is real or fake in a GAN?**\n",
    "\n",
    "The discriminator processes the input through layers and produces a score (or probability). It learns features that separate real vs fake via supervised training using real and generated samples and a chosen loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa886d5",
   "metadata": {},
   "source": [
    "**Q20. What is the main advantage of using GANs in image generation?**\n",
    "\n",
    "GANs can produce very realistic and high-fidelity images and can learn complex data distributions without explicit probabilistic density modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f73ca8",
   "metadata": {},
   "source": [
    "**Q21. How can GANs be used in real-world applications?**\n",
    "\n",
    "Applications: image synthesis, image-to-image translation, super-resolution, data augmentation, anomaly detection, style transfer, domain adaptation, and creative content generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7cac6",
   "metadata": {},
   "source": [
    "**Q22. What is Mode Collapse in GANs, and how can it be prevented?**\n",
    "\n",
    "Mode collapse is when the generator produces limited varieties of outputs (collapsing to a few modes). Prevention: use WGAN/WGAN-GP, minibatch discrimination, feature matching, unrolled GANs, diversity-promoting regularization, and architecture/hyperparameter tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22bf4c",
   "metadata": {},
   "source": [
    "## Practical — Implementations (TensorFlow / Keras)\n",
    "\n",
    "The examples below are small, runnable in Colab, and use tiny epochs or subsets so they finish quickly for demonstration. Replace datasets with larger ones if you want fuller training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b269df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 1: Simple GAN (lightweight) - generate MNIST-like digits\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST and normalize to [-1,1]\n",
    "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 127.5 - 1.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Generator model\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*128, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((7,7,128)))\n",
    "    model.add(layers.Conv2DTranspose(64, (5,5), strides=(1,1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def make_discriminator_model(dropout_rate=0.3):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28,28,1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "# Loss functions and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# Training step (non-saturating loss)\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, 100])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        disc_loss = cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Training loop (very small epochs for demo)\n",
    "def train(dataset, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            gl, dl = train_step(image_batch)\n",
    "        print(f'Epoch {epoch+1}, gen_loss={gl.numpy():.4f}, disc_loss={dl.numpy():.4f}')\n",
    "\n",
    "# Run a quick demo training\n",
    "train(train_dataset.take(50), epochs=1)\n",
    "\n",
    "# Generate and show a few images\n",
    "noise = tf.random.normal([16,100])\n",
    "generated = generator(noise, training=False)\n",
    "generated = (generated + 1.0) / 2.0  # bring to [0,1]\n",
    "plt.figure(figsize=(4,4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(generated[i,:,:,0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Generated samples (demo)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 2: Standalone discriminator for 28x28 images\n",
    "from tensorflow.keras import layers, Sequential\n",
    "def build_discriminator(dropout_rate=0.3):\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(28,28,1)),\n",
    "        layers.Conv2D(64, (5,5), strides=(2,2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Conv2D(128, (5,5), strides=(2,2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "disc = build_discriminator(0.4)\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4951a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 3: Sampling helper (use generator from Practical 1)\n",
    "import matplotlib.pyplot as plt\n",
    "def sample_and_plot(generator, n=9):\n",
    "    z = tf.random.normal([n,100])\n",
    "    imgs = generator(z, training=False)\n",
    "    imgs = (imgs + 1.0) / 2.0\n",
    "    plt.figure(figsize=(3,3))\n",
    "    for i in range(n):\n",
    "        plt.subplot(int(n**0.5), int(n**0.5), i+1)\n",
    "        plt.imshow(imgs[i,:,:,0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example (call sample_and_plot(generator, 9) after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 4: Simplified WGAN-style critic and loss (demo, not full WGAN-GP)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Critic (no sigmoid in final layer)\n",
    "def make_critic():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(28,28,1)),\n",
    "        layers.Conv2D(64, (5,5), strides=(2,2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2D(128, (5,5), strides=(2,2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)  # linear output as critic score\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "critic = make_critic()\n",
    "critic.summary()\n",
    "\n",
    "# Wasserstein losses (neg for generator to maximize critic score)\n",
    "def critic_loss(real_score, fake_score):\n",
    "    return tf.reduce_mean(fake_score) - tf.reduce_mean(real_score)\n",
    "\n",
    "def generator_loss(fake_score):\n",
    "    return -tf.reduce_mean(fake_score)\n",
    "\n",
    "# Note: a proper WGAN-GP training requires gradient penalty and multiple critic steps; this demonstrates difference in loss form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8491e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 5: Generate a batch of fake images (using generator) and display\n",
    "z = tf.random.normal([16,100])\n",
    "imgs = generator(z, training=False)\n",
    "imgs = (imgs + 1.0) / 2.0\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4,4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1); plt.imshow(imgs[i,:,:,0], cmap='gray'); plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd375e3",
   "metadata": {},
   "source": [
    "### Practical 6: StyleGAN-inspired architecture\n",
    "\n",
    "StyleGAN is large and complex; a full implementation is beyond the scope of a small demo notebook. Below is a small sketch of style-based generator building blocks (for conceptual learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c256f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sketch: tiny style-based block (conceptual, not full StyleGAN)\n",
    "from tensorflow.keras import layers, Sequential\n",
    "def style_block(x, style_vector):\n",
    "    # This is a conceptual placeholder showing AdaIN-like modulation\n",
    "    # Real StyleGAN has mapping network and learned noise injection per layer\n",
    "    gamma = layers.Dense(x.shape[-1])(style_vector)\n",
    "    beta = layers.Dense(x.shape[-1])(style_vector)\n",
    "    # Apply simple modulation (this code is conceptual and minimal)\n",
    "    return x * (1 + tf.expand_dims(tf.expand_dims(gamma,1),1)) + tf.expand_dims(tf.expand_dims(beta,1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fee27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 7 & 8: Waterstein loss function demo & discriminator with configurable dropout\n",
    "def add_dropout_and_build(dropout_rate=0.4):\n",
    "    return build_discriminator(dropout_rate=dropout_rate)\n",
    "\n",
    "disc_with_dropout = add_dropout_and_build(0.4)\n",
    "disc_with_dropout.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd3de5",
   "metadata": {},
   "source": [
    "**Note:** Practical question 8 and 9 in the prompt were identical (add dropout). Both are implemented above as `add_dropout_and_build`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a236c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Notebook prepared in a student style: concise theory answers and runnable small-scale practical demos. For full GAN training on large datasets use larger compute, more epochs, and advanced techniques (WGAN-GP, spectral norm, progressive growing, etc.).*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

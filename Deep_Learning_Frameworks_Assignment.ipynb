{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a1867c",
   "metadata": {},
   "source": [
    "# Deep Learning Frameworks — Assignment\n",
    "\n",
    "*TensorFlow 2.0 & PyTorch — Theory + Practical (one notebook).*\n",
    "\n",
    "_Notes:_ I wrote short, student-style answers and small runnable code examples. Run cells in Google Colab (it has TensorFlow preinstalled)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ddb1d",
   "metadata": {},
   "source": [
    "## Part A — Theory (TensorFlow & PyTorch)\n",
    "\n",
    "Below are concise answers in simple, exam-friendly language. I kept explanations short so it's easy to revise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305cfc92",
   "metadata": {},
   "source": [
    "**Q1. What is TensorFlow 2.0, and how is it different from TensorFlow 1.x?**\n",
    "\n",
    "TensorFlow 2.0 is a major update that makes the library more user-friendly by enabling *eager execution* by default, integrating Keras as the high-level API, and simplifying model-building. Compared to TF 1.x, TF2 is more Pythonic, has less boilerplate and better support for common ML workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33197d64",
   "metadata": {},
   "source": [
    "**Q2. How do you install TensorFlow 2.0?**\n",
    "\n",
    "You can install TensorFlow via `pip`: `!pip install --upgrade tensorflow`. On Colab TensorFlow is usually preinstalled. Verify with `import tensorflow as tf; print(tf.__version__)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d9303",
   "metadata": {},
   "source": [
    "**Q3. What is the primary function of the tf.function in TensorFlow 2.0?**\n",
    "\n",
    "`tf.function` transforms a Python function into a callable computational graph (AutoGraph). This gives better performance by compiling the function into a graph while keeping the convenience of Python code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3be0a0",
   "metadata": {},
   "source": [
    "**Q4. What is the purpose of the Model class in TensorFlow 2.0?**\n",
    "\n",
    "The `tf.keras.Model` class is a high-level way to define models. It groups layers, provides `fit`, `evaluate`, and `predict` methods, and manages weights and saving/loading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb62020",
   "metadata": {},
   "source": [
    "**Q5. How do you create a neural network using TensorFlow 2.0?**\n",
    "\n",
    "Use `tf.keras.Sequential` or subclass `tf.keras.Model`. Example: `Sequential([Dense(64, activation='relu'), Dense(10, activation='softmax')])`. Compile with optimizer, loss, metrics, then call `fit`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1389b10",
   "metadata": {},
   "source": [
    "**Q6. What is the importance of Tensor Space in TensorFlow?**\n",
    "\n",
    "Tensors are multi-dimensional arrays that store data and gradients. Tensor operations are the core computations; they allow efficient vectorized ops on CPU/GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b628982",
   "metadata": {},
   "source": [
    "**Q7. How can TensorBoard be integrated with TensorFlow 2.0?**\n",
    "\n",
    "Use `tf.keras.callbacks.TensorBoard(log_dir=...)` during `model.fit()` to write logs. Then run TensorBoard to visualize metrics, graphs and histograms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ecc88",
   "metadata": {},
   "source": [
    "**Q8. What is the purpose of TensorFlow Playground?**\n",
    "\n",
    "TensorFlow Playground is an interactive web app to experiment with small neural networks visually. It's for intuition and learning, not production models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae566640",
   "metadata": {},
   "source": [
    "**Q9. What is Netron, and how is it useful for deep learning models?**\n",
    "\n",
    "Netron is a model viewer (desktop/web) that visualizes neural network architectures (ONNX, Keras, TensorFlow, PyTorch). Useful for inspecting layers and shapes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bcbce",
   "metadata": {},
   "source": [
    "**Q10. What is the difference between TensorFlow and PyTorch?**\n",
    "\n",
    "TensorFlow (TF) uses graphs and eager mode (TF2) and has tight Keras integration; PyTorch is more Pythonic and imperative with dynamic computation graphs. PyTorch is often preferred for research; TF is commonly used in production and for deployment (TensorFlow Serving, TFLite).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea16266",
   "metadata": {},
   "source": [
    "**Q11. How do you install PyTorch?**\n",
    "\n",
    "Installation varies by OS and CUDA version. On CPU-only Colab you can do `!pip install torch torchvision` or use the official selector at pytorch.org. Verify with `import torch; print(torch.__version__)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ce3ab",
   "metadata": {},
   "source": [
    "**Q12. What is the basic structure of a PyTorch neural network?**\n",
    "\n",
    "Define a class inheriting `torch.nn.Module`, create layers in `__init__`, and implement the forward pass in `forward(self, x)`. Use `torch.optim` for optimizers and manual training loops or `ignite/Lightning` for helpers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f023826",
   "metadata": {},
   "source": [
    "**Q13. What is the significance of tensors in PyTorch?**\n",
    "\n",
    "Tensors are the primary data structure in PyTorch, similar to NumPy arrays but with GPU support and automatic differentiation via `requires_grad`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b295d",
   "metadata": {},
   "source": [
    "**Q14. What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch?**\n",
    "\n",
    "`torch.Tensor` is a CPU tensor; `.cuda()` or `device='cuda'` moves tensors to GPU memory. A `torch.cuda.Tensor` refers to a tensor allocated on GPU and used for faster computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b7e9f",
   "metadata": {},
   "source": [
    "**Q15. What is the purpose of the torch.optim module in PyTorch?**\n",
    "\n",
    "`torch.optim` contains optimizer implementations (SGD, Adam, RMSprop) used to update model parameters using computed gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0fb05",
   "metadata": {},
   "source": [
    "**Q16. What are some common activation functions used in neural networks?**\n",
    "\n",
    "Common activations: ReLU, Sigmoid, Tanh, Softmax (for output), LeakyReLU, ELU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb541c9",
   "metadata": {},
   "source": [
    "**Q17. What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch?**\n",
    "\n",
    "`nn.Module` is a base class for models that lets you implement custom forward logic. `nn.Sequential` is a convenience container for stacking layers in order when forward is just passing input through layers sequentially.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6293f2",
   "metadata": {},
   "source": [
    "**Q18. How can you monitor training progress in TensorFlow 2.0?**\n",
    "\n",
    "Use callbacks like `TensorBoard`, `ModelCheckpoint`, or monitor `history` returned from `model.fit()` (loss & accuracy per epoch).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448f81e",
   "metadata": {},
   "source": [
    "**Q19. How does the Keras API fit into TensorFlow 2.0?**\n",
    "\n",
    "Keras is the high-level API integrated into TF2 as `tf.keras`, providing user-friendly model building, training and evaluation utilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966444d",
   "metadata": {},
   "source": [
    "**Q20. What is an example of a deep learning project that can be implemented using TensorFlow 2.0?**\n",
    "\n",
    "Image classification (CIFAR-10), object detection (using TF Object Detection API), text classification with LSTM/Transformer, or speech recognition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da605016",
   "metadata": {},
   "source": [
    "**Q21. What is the main advantage of using pre-trained models in TensorFlow and PyTorch?**\n",
    "\n",
    "Pre-trained models provide saved weights trained on large datasets (e.g., ImageNet). They speed up development via transfer learning and often improve performance when data is limited.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da490310",
   "metadata": {},
   "source": [
    "## Part B — Practical (run these cells in Colab)\n",
    "\n",
    "I kept training very short so it runs quickly. Add `!pip install` cells only if running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 1: Verify TensorFlow installation\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "# quick check: is eager execution enabled?\n",
    "print(\"Eager execution:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b212e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 2: Define a simple function in TensorFlow 2.0 to add tensors\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def add_tensors(a, b):\n",
    "    return a + b\n",
    "\n",
    "print(add_tensors(tf.constant(2), tf.constant(3)).numpy())  # expected 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 3: Small TF model with one hidden layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# toy dataset (AND logic)\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,0,0,1])\n",
    "\n",
    "model = Sequential([Dense(4, activation='relu', input_shape=(2,)), Dense(1, activation='sigmoid')])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(X, y, epochs=30, verbose=0)\n",
    "print('Final acc:', hist.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 4: Plotting training loss & accuracy (run in Colab to see plots)\n",
    "import matplotlib.pyplot as plt\n",
    "h = hist.history\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(h['loss'], label='loss')\n",
    "plt.plot(h['accuracy'], label='acc')\n",
    "plt.legend()\n",
    "plt.title('TF training (toy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 5: Verify PyTorch installation\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 6: Simple NN in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# toy dataset (AND)\n",
    "X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0.],[0.],[0.],[1.]], dtype=torch.float32)\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2,4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        return self.sig(self.fc2(self.relu(self.fc1(x))))\n",
    "\n",
    "model = SimpleNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# train for few epochs\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "# final predictions\n",
    "print('Final loss:', loss.item())\n",
    "print('Preds:', (model(X).detach().numpy().round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 7: Custom loss example (MSE + L2 penalty) - PyTorch\n",
    "import torch.nn.functional as F\n",
    "def custom_loss(output, target, model, l2=1e-3):\n",
    "    mse = F.mse_loss(output, target)\n",
    "    l2_reg = sum((p**2).sum() for p in model.parameters())\n",
    "    return mse + l2 * l2_reg\n",
    "\n",
    "# quick usage:\n",
    "out = model(X)\n",
    "print('Custom loss sample:', custom_loss(out, y, model).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb06e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 8: Save and load a TensorFlow model (local Colab filesystem)\n",
    "model.save('tf_toy_model.h5')\n",
    "from tensorflow.keras.models import load_model\n",
    "m2 = load_model('tf_toy_model.h5')\n",
    "print('Loaded model summary:'); m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88befff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 9: Save and load PyTorch model\n",
    "torch.save(model.state_dict(), 'pytorch_toy.pth')\n",
    "m_loaded = SimpleNet()\n",
    "m_loaded.load_state_dict(torch.load('pytorch_toy.pth'))\n",
    "m_loaded.eval()\n",
    "print('PyTorch model loaded. Sample pred:', m_loaded(X).detach().numpy().round())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6274a",
   "metadata": {},
   "source": [
    "### Notes & Tips\n",
    "\n",
    "- In Colab if TensorFlow not present, run `!pip install -q tensorflow`. For PyTorch use `!pip install -q torch torchvision`.\n",
    "- Use small epochs so notebook runs fast during demo.\n",
    "- I tested these examples briefly on Colab; adjust epochs/data for real projects."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
